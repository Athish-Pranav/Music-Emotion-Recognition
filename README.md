
# Music Emotion Recognition
An audio-based deep learning project aimed at classifying music into emotional categories using spectrogram data and a 2D CNN model. This project leverages Russell’s Circumplex Model of Affect to classify songs into specific emotions (Happy, Sad, Bored) based on arousal and valence values.

# Introduction
This project tackles the challenge of Music Emotion Recognition (MER) by classifying music tracks into emotional categories based on Russell’s Circumplex Model of Affect. By processing audio features with a 2D Convolutional Neural Network (CNN), the system identifies patterns within spectrogram representations to recognize emotions. The dataset used includes music in multiple languages (Hindi, Turkish, Malay, English), allowing the model to learn across cultural and linguistic contexts.

Motivation: Music plays a significant role in expressing and influencing emotions. Building a model that recognizes music-induced emotions could enhance applications like music recommendation systems, mood-based playlist generators, and interactive media.

# Features
Multilingual Dataset: Processes music in Hindi, Turkish, Malay, and English, providing a culturally diverse range of emotions.
2D CNN Architecture: Uses 2D CNNs to analyze spectrograms, capturing both temporal and frequency-based emotional cues in the audio.
Emotion Classification: Categorizes music into emotional states (Happy, Sad, Bored) using Russell’s model based on arousal and valence.
Feature-Rich Data: Incorporates a variety of audio features like MFCCs, Mel spectrograms, and top selected features for improved accuracy.

# Results
The model achieved an accuracy of 95% on the test dataset, demonstrating robust performance in recognizing emotional categories across different languages and music genres.

![image](https://github.com/user-attachments/assets/903570ba-7379-44ce-8bde-fe503f58113f)
![image](https://github.com/user-attachments/assets/5890682c-90b6-4f55-80aa-f185a634dda1)
